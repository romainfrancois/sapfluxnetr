---
title: "sapfluxnetr Quick Guide"
author: "VÃ­ctor Granda (Sapfluxnet Team)"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{sapfluxnetr Quick Guide}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: inline
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

folder <- 'resources'
```

`sapfluxnetr` R package provides tools for a tidy data analysis for the first
sap flow measurements global database 
([Sapfluxnet Project](http://sapfluxnet.creaf.cat/app)). In this vignette you
will learn how to install the package, download the data and get started with
some data metrics.

## Installing the package

`sapfluxnetr` stable versions are in the CRAN repository, so the installation of
the package is as per usual:

```{r cran_inst, eval=FALSE}
install.packages('sapfluxnetr')
```

Development versions of the package reside in github. If you want the lastest
updates (and also the latests bugs, be advised ;) ), please install the devel
branch of the [github repository](https://github.com/sapfluxnet/sapfluxnetr)
with the `devtools` package:

```{r github_inst, eval=FALSE}
devtools::install_github('sapfluxnet/sapfluxnetr', ref = 'master',
                         build_vignettes = TRUE)
```

Now you can load the package, and also the `tidyverse` package, as it will be
needed later:

```{r pkg_load}
library(sapfluxnetr)
library(tidyverse)
```


## Download the data

### Zenodo

To be written as the data is not yet in Zenodo

### Sapfluxnet Data test suite

Download the test suite data from [here](). Unzip the file maintaining the
folder structure:

```
sapfluxnet_db
  |
  |-- test_suite
       |
       |-- plant
       |-- sapwood
       |-- leaf
```

`plant` folder contains the sites with sap flow plant level units available.  
`sapwood` folder contains the sites with sap flow sapwood level units available.  
`leaf` folder contains the sites with sap flow leaf level units available.

To start working with the data simply create an RStudio project in the
`sapfluxnet_db` folder and follow the following examples.

>DISCLAIMER: In order to be able to build the vignette in the CRAN tests
 the following examples will be based on a small subset of Sapfluxnet Data,
 composed by `ARG_TRE`, `ARG_MAZ` and `AUS_CAN_ST2_MIX` sites.
 Outputs may vary if you follow the vignette examples with the complete database.

## Inspecting a site

First, let's get used to the data structure that Sapfluxnet Data provides, and
for that we will choose a site and start playing with it.

### Loading a site

In this example we will use the `ARG_MAZ` site, as it is small and it will be
fast to see the package capabilities. There are sites like `FRA_PUE` 20 times
bigger than this, but as you can imagine, the time is also increasing when
analising those bigger datasets.  
So, let's read the data in the environment:

```{r read_and_inspect, eval=FALSE}
# read the data
arg_maz <- read_sfn_data('ARG_MAZ', folder = 'test_suite/plant')

# see a brief summary of the site:
arg_maz
```

```{r read_internals, echo=FALSE}
# read the data
arg_maz <- read_sfn_data('ARG_MAZ', folder = folder)

# see a brief summary of the site:
arg_maz
```


At first glance, we know by this summary that is an Argentinian forest (first
three letters of the site code are always the country code),
contributed by Sebastian Pfautsch and Pablo Peri with 5 *Nothofagus pumilio*
trees measured along 15 days in 2009. Also we can see the environmental variables
measured (`ta`, `rh`, `vpd`, `sw_in`, `ws`, `precip`, `swc_shallow`, `ppfd_in`
and `ext_rad`) and the biome classification. Finally, we can see that the
environmental data has some flags (more on that later).

### Accesing the data and the metadata

`sfn_class` objects have different slots containing the different data, each of
one has an accesor function (see `?sfn_get_methods` and
`vignette('sfn-data-classes', package = 'sapfluxnetr'` for detailed info):

```{r accesors_data}
# sapf data with original site timestamp
arg_maz_sapf <- get_sapf_data(arg_maz, solar = FALSE)
arg_maz_sapf

# env_data with calculated aparent solar time
arg_maz_env <- get_env_data(arg_maz, solar = TRUE)
arg_maz_env
```

You can see that the TIMESTAMP variable changes between both kinds of data. That
is because the TIMESTAMP returned is controled by the `solar` parameter
(see `?sfn_get_methods`).

Metadata can be accesed in the same way:

```{r accesors_md}
arg_maz_site_md <- get_site_md(arg_maz)
arg_maz_site_md
arg_maz_stand_md <- get_stand_md(arg_maz)
arg_maz_stand_md
arg_maz_species_md <- get_species_md(arg_maz)
arg_maz_species_md
arg_maz_plant_md <- get_plant_md(arg_maz)
arg_maz_plant_md
arg_maz_env_md <- get_env_md(arg_maz)
arg_maz_env_md
```

If in doubt about some of the metadata variables (what it means, units...) a
description can be obtained from `describe_md_variable` function:

```{r describe}
# what is env_ta?
describe_md_variable('env_ta')

# or pl_species?
describe_md_variable('pl_species')
```

There is also some accessors that can come in handy sometimes. `get_timestamp` and
`get_solar_timestamp` access to the original timestamp and the apparent solar
time timestamp. `get_si_code` access to the site code. See
`vignette('sfn-data-classes', package = 'sapfluxnetr'` for more info.

#### Flags

`sfn_data` objects also have two more slots, accesed with `get_sapf_flags` and
`get_env_flags`.

```{r get_flags}
arg_maz_sapf_flags <- get_sapf_flags(arg_maz, solar = TRUE)
arg_maz_sapf_flags

arg_maz_env_flags <- get_env_flags(arg_maz, solar = TRUE)
arg_maz_env_flags
```

This datsets store any flag that each data point may have (possible outlier,
data removed in the Quality Check of the data...). For a complete list of
flags possible values see `vignette('data-flags', package = 'sapfluxnetr')`.
As an example, let's see which values are marked as "RANGE_WARN" (a warning
indicating that the value may be out of normal variable range):

```{r out_range}
arg_maz_env_flags %>%
  filter_all(any_vars(stringr::str_detect(., 'RANGE_WARN')))
```

We see that the out of range warnings refer to wind variable. We can cross the
data to see which values of wind speed are giving the warnings,

```{r crossing_flags}
arg_maz_env_flags %>%
  filter_all(any_vars(stringr::str_detect(., 'RANGE_WARN'))) %>%
  semi_join(arg_maz_env, ., by = 'TIMESTAMP') %>%
  select(TIMESTAMP, ws)
```

and confirm that the warnings refer to values above the "usual" wind speed
maximum.

### Plotting an object

We can also plot the different data with the help of `sfn_plot` function. It will
return `ggplot` objects that can be modified afterwards:

```{r sapf_plot, fig.width=6}
sfn_plot(arg_maz, type = 'sapf', solar = TRUE) +
  facet_wrap(~ Tree) + theme(legend.position = 'none')
```

```{r env_plot, fig.width=6, fig.height=4}
sfn_plot(arg_maz, type = 'env', solar = TRUE) +
  facet_wrap(~ Variable, scales = 'free_y') + theme(legend.position = 'none')
```

We can also plot environmental variables individually (with the `type` argument),
or an environmental variable *versus* the sap flow measurements (with the
`formula` argument). See `?sfn_plot` for a complete description of the
available plots.

```{r vpd_and_vs, fig.show='hold'}
# vpd individually
sfn_plot(arg_maz, type = 'vpd', solar = TRUE)
# vpd vs sapf
sfn_plot(arg_maz, formula = ~vpd, solar = TRUE) +
  theme(legend.position = 'none')
```

### Aggregate metrics

Sapfluxnet data is stored as subdaily measures, being the timestep different
among sites (it can be from 10 minutes in some sites to 1 hour in others).  
`sapfluxnetr` offers some simple, yet complete aggregation functions returning
some pre-defined metrics: `daily_metrics`, `monthly_metrics` and
`nightly_metrics`.

`daily_metrics` and `monthly_metrics` returns a list containing the sapflow and
environmental data aggregated by day and month, respectively, in three different
flavours, *general* (all data summarised), *pd* (only predawn period summarised)
and *md* (only midday period summarised).  
`nightly_metrics` divides the data in *day* and *night* and returns the metrics
for each subset in the period declared (daily or monthly).  
All three funtions return the same metrics, *mean*, *standard deviation*, *n*,
*data coverage*, *quantiles* (0.95 and 0.99 by default), *diurnal centroid*,
*min value*, *time at min value*, *max value* and *time at max value*.

Let's see some examples:

```{r daily}
arg_maz_daily <- daily_metrics(arg_maz, solar = TRUE)

names(arg_maz_daily)
names(arg_maz_daily[['sapf']])
names(arg_maz_daily[['env']])
```

We can see that results are divided in `sapf` and `env` and inside each of them
there are the `gen`, `pd` and `md` results. So if we want to inspect the
general aggregation for sap flow measures we can do it as follows:

```{r daily_sapf_gen}
arg_maz_daily[['sapf']][['sapf_gen']]
```

We can also select specific variables, for example the 0.99 quantiles of sap
flow measures:

```{r daily_sapf_gen_q99}
arg_maz_daily[['sapf']][['sapf_gen']] %>%
  select(TIMESTAMP, ends_with('q_99'))
```

The same is applicable to the environmental data, in this case we can select
mean values of the midday interval for environmental variables:

  > Note that in the midday and predawn intervals metrics, the variable names
    end with `_md` and `_pd` respectively

```{r daily_sapf_md_mean}
arg_maz_daily[['env']][['env_md']] %>%
  select(TIMESTAMP_md, ends_with('mean_md')) # note that midday variables ends with "_md"
```

If interested in custom metrics or custom aggregations, there is a generic
function, `sfn_metrics` that allows for customization of the statistics to
calculate and the periods to aggregate. See `?sfn_metrics` and
`vignette('custom-aggregation'. package = 'sapfluxnetr')` for more details
about it.

#### Agregation TIMESTAMP format

It's worth to mention that aggregated TIMESTAMPS are fixed to the **beginning**
of the period aggregated, meaning that data from `2018-01-01 00:00:00` to
`2018-01-01 23:59:59` are aggregated as `2018-01-01 00:00:00`.  
You can change this using the `side` parameter (see `?sfn_metrics`)

## Working with multiple sites

Getting the insights about one site is interesting, but getting the insights
of a common group of sites could be even more interesting. `sapfluxnetr` allows
filtering sites by metadata values (biomes, countries, species...) and work
with them as a unique set.

### Building the metadata database

First thing we have to do is creating a metadata database. It is not mandatory,
but filtering sites by metadata can be a very time and resources consuming step
if we have to temporary build the database each time we want filter sites. So,
let's create a cached metadata database. This will take some minutes, so maybe
it is a good moment to prepare a hot beverage ;)

```{r metadata_database, eval = FALSE}
sfn_metadata <- read_sfn_metadata(folder = 'test_suite/plant', .write_cache = TRUE)
```

```{r metadata_database_real, echo=FALSE, warning=FALSE}
# sfn_metadata <- read_sfn_metadata(folder = folder, .write_cache = TRUE)
sfn_metadata <- sapfluxnetr:::.write_metadata_cache(folder = folder, .dry = TRUE)
```


The important bit here is `.write_cache = TRUE`. This will write a file called
`.metadata_cache.RData` containing all the metadata for all sites present in
`folder`. This file will be used any time we will filter the metadata, so there
is no need of accessing all the data again.  
If we take a look at `sfn_metadata` we can see a list with 5 data frames, one for
each metadata class (site, stand, species, plant and environmental metadata).

```{r md_db_str}
# access plant metadata
sfn_metadata[['plant_md']]
```

### Filtering sites

Now that we have our metadata database built, we can start filtering sites by
metadata, with the `filter_by_var` function. As a first try, let's list all
sites belonging to temperate forest biome (mediterranean included):

```{r filtering_by_md, eval=FALSE}
temperate <- filter_by_var(
  si_biome %in% c('Mediterranean', 'Temperate forest'),
  folder = 'test_suite/plant',
  .use_cache = TRUE
)

temperate
```

```{r filtering_by_md_real, echo=FALSE, warning=FALSE}
temperate <- c("ARG_MAZ", "ARG_TRE", "AUS_CAN_ST2_MIX")
temperate
```

You can combine all filters you want:

```{r filters_combined, eval=FALSE}
temperate_hr <- filter_by_var(
  si_biome %in% c('Mediterranean', 'Temperate forest'),
  pl_sap_meth == 'HR',
  folder = 'test_suite/plant',
  .use_cache = TRUE
)
temperate_hr
```

```{r filters_combined_real, echo=FALSE, warning=FALSE}
temperate_hr <- c("ARG_MAZ", "ARG_TRE")
temperate_hr
```

Remember that you can get all the info from a metadata variable with
`describe_md_variable`, and also you can get a complete list of metadata
variables for filtering with `sfn_vars_to_filter`:

```{r vars_to_filter}
sfn_vars_to_filter()

# and see what values we must use for filtering by pl_sens_meth
describe_md_variable('pl_sens_meth')
```


### sfn_data_multi objects

We can load all temperate sites with a simple pipe

```{r multi, eval=FALSE}
temperate_sites <- temperate %>%
  read_sfn_data(folder = 'test_suite/plant')
temperate_sites
```

```{r multi_real, echo=FALSE}
temperate_sites <- temperate %>%
  read_sfn_data(folder = folder)
temperate_sites
```

We have created a `sfn_data_multi` object, which is just a list, but adapted to
contain `sfn_data` objects. Main functions of `sapfluxnetr` are adapted to work
with this lists. As any list, we can access the sites:

```{r mutil_site}
temperate_sites[['AUS_CAN_ST2_MIX']] # same as temp_hr_sites[[3]]
```

### Multi aggregation

Now we can aggregate all sites at once

```{r multi_aggregation}
temperate_aggregated <- temperate_sites %>%
  daily_metrics()
```

and *voilÃ *, all sites aggregated, so we can start working with the data.

The structure of `daily_metrics` results allows for easy data subsetting and
is fast to create a data frame with the desired data. In this example we will
join all general metrics of all sites and try to see if there is site effect in
the relationship between sapflow and vpd, using the 0.95 quantiles as maximum
values of sapflow.

First we join all the sap flow measurements. As we need to acces to a nested
list we can use the `map` function from `purrr` package (included in `tidyverse`).
`map` maps a function to all elements in a list-like structure. In this case we
provide a character vector to extract all nested elements called `'sapf_gen'`.
And after that we apply to all extracted elements the `select` function to
retrieve only the TIMESTAMP and all variables ending in `'_q_95'`.

```{r ex_1_sapf}
sapf_data <- temperate_aggregated %>%
  map(c('sapf', 'sapf_gen')) %>%
  map(~ select(., TIMESTAMP, ends_with('_q_95')))

sapf_data
```

We do the same for the environmental data, in this case extracting the mean
metric and selecting only the vpd variable

```{r ex_1_env}
env_data <- temperate_aggregated %>%
  map(c('env', 'env_gen')) %>%
  map(~ select(., TIMESTAMP, ends_with('_mean'))) %>%
  map(~ select(., TIMESTAMP, starts_with('vpd_')))

env_data
```

We will need the plant metadata to access the sapwood area data for each tree
of temperate sites

```{r ex_1_plant_md}
plant_md <- sfn_metadata[['plant_md']] %>%
  filter(si_code %in% names(temperate_aggregated))

plant_md
```

And now we are ready to put all together, again mostly using the `map` function
to apply to all sites the same functions. This will be done in steps:

1. Join sapf_data and env_data by TIMESTAMP values
1. Gather all sap flow measurements in one column to be able to color/fill/group
   by individual tree and also join all sites in one data set
1. Convert to tibble (to bind the rows)
1. Bind rows from all sites in one data set
1. Create the plant code (`pl_code`) based on the Tree column (tree name without
   the metric ending, `_q_95` in this case)
1. Order by TIMESTAMP
1. Join the plant metadata

```{r ex_1_whole_data}
whole_data <- sapf_data %>%
  map2(env_data, ~ full_join(.x, .y, by = 'TIMESTAMP')) %>% # step 1
  map(~ gather(., Tree, Sapflow, -TIMESTAMP, -starts_with('vpd_'))) %>% # step 2
  map(as_tibble) %>% # step 3
  bind_rows() %>% # step 4
  mutate(pl_code = str_remove_all(Tree, '_q_95')) %>% # step 5
  arrange(TIMESTAMP) %>% # step 6
  left_join(plant_md, by = 'pl_code') # step 7

whole_data
```


No we can plot to see the relationship between vpd and sap flow measurements,
per sapwood area and site:

```{r ex_1_plot, fig.width=7, fig.height=4.3}
ggplot(whole_data, aes(x = vpd_mean, y = Sapflow, colour = si_code,
                       size = pl_sapw_area)) +
  geom_point(alpha = 0.2)
```

